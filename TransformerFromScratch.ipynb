{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMgq8pcX8wlTNdUCmNk2HCz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sysgenerated/transformer-from-scratch/blob/main/TransformerFromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Transformer From Scratch\n",
        "\n",
        "This code replicates a BERT transformer using standard PyTorch modules.\n",
        "\n",
        "It includes the following sections:\n",
        "0. Import Libraries\n",
        "1. Initialize Variables\n",
        "2. Create Data Loader\n",
        "3. Load Dataset\n",
        "4. Create Transformer\n",
        "5. Create Model\n",
        "6. Train Model\n",
        "7. Evaluate Model"
      ],
      "metadata": {
        "id": "5c2w6XW0JF03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Import Libraries"
      ],
      "metadata": {
        "id": "noUpdSNVYQUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from os.path import exists\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import requests\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import math\n",
        "import re"
      ],
      "metadata": {
        "id": "W9hmSWLoYP7k"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Initialize Variables"
      ],
      "metadata": {
        "id": "3L4SymrzY8mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initializing...\")\n",
        "\n",
        "n_vocab = 40000                 # Number of tokens (words = tokens) used\n",
        "batch_size = 128                # Number of tokens processed in one forward pass\n",
        "seq_len = 20                    # Maximum number of tokens allowed in a sentence\n",
        "n_encoders = 8                  # Number of stacked encoders\n",
        "n_heads = 8                     # Number of attention heads within each encoder\n",
        "embed_size = 128                # Length of word embeddings\n",
        "inner_ff_size = embed_size * 4  # Size of vector passed between stacked encoders\n",
        "dropout = 0.1                   # Percent of neurons dropped out in training\n",
        "n_workers = 12\n",
        "\n",
        "optim_kwargs = {'lr':2e-3, 'weight_decay':1e-4, 'betas':(.9,.999)}\n",
        "\n",
        "n_iteration = 30000             # Number of training iterations\n",
        "print_each = 5                  # Print training results modulo this number\n"
      ],
      "metadata": {
        "id": "CvS-fAvHZAo0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c084a4b1-fe72-4a37-9f3c-836e631d5fb6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create Data Loader"
      ],
      "metadata": {
        "id": "Xs9sJuZzZBAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentencesDataset(Dataset):\n",
        "    def __init__(self, sentences, vocab, seq_len):\n",
        "        dataset = self\n",
        "\n",
        "        dataset.sentences = sentences\n",
        "        dataset.vocab = vocab + ['<ignore>', '<oov>', '<mask>']\n",
        "        dataset.vocab = {e:i for i, e in enumerate(dataset.vocab)}    # Create a lookup list for tokens (words); word = int\n",
        "        dataset.rvocab = {v:k for k,v in dataset.vocab.items()}       # Create a reverse lookup list for integers; int = word\n",
        "        dataset.seq_len = seq_len                                     # Maximum number of tokens allowed in a sentence\n",
        "\n",
        "        dataset.IGNORE_IDX = dataset.vocab['<ignore>']                # Replacement tag for tokens to ignore\n",
        "        dataset.OUT_OF_VOCAB_IDX = dataset.vocab['<oov>']             # Replacement tag for unknown words\n",
        "        dataset.MASK_IDX = dataset.vocab['<mask>']                    # Replacement tag for the masked word prediction task\n",
        "\n",
        "    def __getitem__(self, index, p_random_mask=0.15):\n",
        "        dataset = self\n",
        "        s = []\n",
        "        while len(s) < dataset.seq_len:\n",
        "            s.extend(dataset.get_sentence_idx(index % len(dataset)))\n",
        "            index += 1\n",
        "\n",
        "        s = s[:dataset.seq_len]                                       # Ensure that the sequence is of length seq_len\n",
        "        [s.append(dataset.IGNORE_IDX) for i in range(dataset.seq_len - len(s))] # Pad retrieved sentence to seq_len using IGNORE_IDX values\n",
        "\n",
        "        s = [(dataset.MASK_IDX, w) if random.random() < p_random_mask else (w, dataset.IGNORE_IDX) for w in s] # Create (masked input : word output) or (word input : ignore output)\n",
        "\n",
        "        return {'input': torch.Tensor([w[0] for w in s]).long(),\n",
        "                'target': torch.Tensor([w[1] for w in s]).long()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def get_sentence_idx(self, index):\n",
        "        dataset = self\n",
        "        s = dataset.sentences[index]\n",
        "        s = [dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in s]\n",
        "        return s\n",
        "\n",
        "\n",
        "def get_batch(loader, loader_iter):\n",
        "    try:\n",
        "        batch = next(loader_iter)\n",
        "    except StopIteration:\n",
        "        loader_iter = iter(loader)\n",
        "        batch = next(loader_iter)\n",
        "    return batch, loader_iter\n"
      ],
      "metadata": {
        "id": "omEzPpdJaE_c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load Dataset"
      ],
      "metadata": {
        "id": "kmaOdnFfaVV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read training data from text file\n",
        "print('loading text...')\n",
        "url = f\"https://raw.githubusercontent.com/sysgenerated/transformer-from-scratch/main/europarl30k.fr.txt\"\n",
        "sentences = requests.get(url).text.lower().split('\\n')\n",
        "\n",
        "# Split sentences into words\n",
        "print('tokenizing sentences...')\n",
        "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
        "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
        "sentences = [[w for w in s if len(w)] for s in sentences]   # [[words in sentence 1] ... [words in sentence n]]\n",
        "\n",
        "\n",
        "# Create a hacky tokenizer that uses word count\n",
        "print('creating/loading vocab...')\n",
        "pth = 'vocab.txt'\n",
        "if not exists(pth):\n",
        "    words = [w for s in sentences for w in s]\n",
        "    vocab = Counter(words).most_common(n_vocab)             # Keep the N most frequent words\n",
        "    vocab = [w[0] for w in vocab]\n",
        "    open(pth, 'w+').write('\\n'.join(vocab))\n",
        "else:\n",
        "    vocab = open(pth).read().split('\\n')\n",
        "\n",
        "\n",
        "# Create dataset and data loader\n",
        "print('creating dataset...')\n",
        "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
        "kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)\n"
      ],
      "metadata": {
        "id": "kdWL9wG-biNJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da126b12-2ead-4e30-be57-1c6c523df582"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading text...\n",
            "tokenizing sentences...\n",
            "creating/loading vocab...\n",
            "creating dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Create Transformer"
      ],
      "metadata": {
        "id": "g0Hw-oiEcDq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standalone function to calculate attention. Input = Q,K,V tensors. Output = One tensor with the same shape as Q=K=V\n",
        "def attention(q, k, v, mask = None, dropout = None):            # q,k,v are all 4d tensors of the same shape; [batch, head, sequence, embedding mod % heads]; eg [128, 8, 20, 16]\n",
        "    scores = q.matmul(k.transpose(-2, -1))                      # Get matrix dot product Q * K^T (applied for each batch); eg [128, 8, 20, 20]\n",
        "    scores /= math.sqrt(q.shape[-1])                            # Scale dot product to get cosine similarity (scales embedding dimension)\n",
        "\n",
        "    scores = scores if mask is None else scores.masked_fill(mask == 0, -1e3)  # Set element value = -1e3 for each entry in mask matrix that is 0; applies a mask to words that shouldn't influence results\n",
        "\n",
        "    scores = F.softmax(scores, dim = -1)                        # Apply softmax to dotproduct similarity dimension; softmax sums to 1 which keeps the new vectors similar in vector length\n",
        "    scores = dropout(scores) if dropout is not None else scores # Randomly set elements of the matrix to 0\n",
        "    output = scores.matmul(v)                                   # Get matrix dot product (scaled QK^T) * V^T\n",
        "    return output                                               # Outputs a 4d tensor [batch, head, sequence, transformed embedding (split by head)]; eg [128, 8, 20, 16]\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, out_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Concatenated linear layers of learnable weights, q|k|v\n",
        "        self.linear = nn.Linear(out_dim, out_dim*3)             # Learnable weights to create q,k,v tensors; eg [128, 128 * 3]\n",
        "\n",
        "        self.n_heads = n_heads                                  # Number of attention heads; eg 8\n",
        "        self.out_dim = out_dim                                  # 128\n",
        "        self.out_dim_per_head = out_dim // n_heads              # 128 // 8 = 16\n",
        "        self.out = nn.Linear(out_dim, out_dim)                  # [128, 128]\n",
        "        self.dropout = nn.Dropout(dropout)                      # 0.1\n",
        "\n",
        "    def split_heads(self, t):\n",
        "        return t.reshape(t.shape[0], -1, self.n_heads, self.out_dim_per_head)\n",
        "\n",
        "    def forward(self, x, y=None, mask=None):                    # x is a 3d tensor of shape [batch, sequence length, embedding size]; eg [128, 20, 128]\n",
        "        y = x if y is None else y                               # In decoder, y comes from encoder. In encoder, y = x\n",
        "\n",
        "        qkv = self.linear(x)                                    # Create 3 transformed versions of X to become q|k|v; [128, 20, 128 * 3]\n",
        "\n",
        "        q = qkv[:, :, :self.out_dim]                            # Extract q|k|v from concatenated matrix [batch, sequence, q__]; [128, 20, 128]\n",
        "        k = qkv[:, :, self.out_dim:self.out_dim*2]              # [batch, sequence, _k_]; [128, 20, 128]\n",
        "        v = qkv[:, :, self.out_dim*2:]                          # [batch, sequence, __v]; [128, 20, 128]\n",
        "\n",
        "        #break into n_heads\n",
        "        q, k, v = [self.split_heads(t) for t in (q,k,v)]        # Reshape 3d to 4d tensor [batches, words, # of heads, # dim per head]; eg: [128, 20, 8, 16]\n",
        "        q, k, v = [t.transpose(1,2) for t in (q,k,v)]           # Transpose words and # of heads [ batches, t(# of heads), t(words), # dim per heads]; eg: [128, 8, 20, 16]\n",
        "\n",
        "        scores = attention(q, k, v, mask, self.dropout)         # Apply attention mechanism to q,k,v; [128, 8, 20, 16]\n",
        "        scores = scores.transpose(1,2).contiguous().view(scores.shape[0], -1, self.out_dim) # Reshape back to 3d tensor; [128, 20, 128]\n",
        "        out = self.out(scores)                                  # Apply another linear transformation to embeddings; [128, 20, 128]\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "J51A42t1cIfi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, inp_dim, inner_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(inp_dim, inner_dim)        # [128, 128 * 4]\n",
        "        self.linear2 = nn.Linear(inner_dim, inp_dim)        # []\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, n_heads, inner_transformer_size, inner_ff_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(n_heads, inner_transformer_size, dropout) # [8, 128, 0.1]\n",
        "        self.ff = FeedForward(inner_transformer_size, inner_ff_size, dropout)   # [128, 128 * 4, 0.1]\n",
        "        self.norm1 = nn.LayerNorm(inner_transformer_size)                       # 128\n",
        "        self.norm2 = nn.LayerNorm(inner_transformer_size)                       # 128\n",
        "        self.dropout1 = nn.Dropout(dropout)                                     # 0.1\n",
        "        self.dropout2 = nn.Dropout(dropout)                                     # 0.1\n",
        "\n",
        "    def forward(self, x, mask=None):                                            # x is a 3d tensor of shape [batch, seq_len, embedding]; [128, 20, 128]\n",
        "        x2 = self.norm1(x)                                                      # Normalize the embedding vector\n",
        "        x = x + self.dropout1(self.mha(x2, mask=mask))                          # Apply attention to the embedding vectors and then add to themselves with dropout\n",
        "        x2 = self.norm2(x)                                                      # Renormalize the embedding vectors\n",
        "        x = x + self.dropout2(self.ff(x2))                                      #\n",
        "        return x"
      ],
      "metadata": {
        "id": "b3JaDvEDcU8t"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, embed_size, max_seq_len = 80):\n",
        "        super().__init__()\n",
        "        self.embed_size = embed_size                              # In this example specifically 128\n",
        "        pe = torch.zeros(max_seq_len, embed_size)                 # In this example specifically [20, 128]\n",
        "        pe.requires_grad = False\n",
        "        for pos in range(max_seq_len):                            # Create sin/cos encoding with shape [max_seq_len, d_model], positional encoding varies across embedding length\n",
        "            for i in range(0, embed_size, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/embed_size)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/embed_size)))\n",
        "        pe = pe.unsqueeze(0)                                      # Convert 2d tensor into 3d tensor of shape [0, max_seq_len, embed_size], [0, 20, 128]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):                                         # The input is a tensor of shape; [128, 20, 128]\n",
        "        return self.pe[:,:x.size(1)]                              # Returns 3d tensor of shape [batch, seq_len, embed_size], [128, 20, 128]\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, n_encoders, n_heads, embed_size, inner_ff_size, n_embeddings, seq_len, dropout=.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embeddings = nn.Embedding(n_embeddings, embed_size)  # In this example specifically [40000, 128]\n",
        "        self.pe = PositionalEmbedding(embed_size, seq_len)        # In this example specifically [0, 128, 20]\n",
        "\n",
        "        encoders = []\n",
        "        for i in range(n_encoders):                                     # Number of stacked encoder heads; eg: 8\n",
        "          encoders += [EncoderLayer(n_heads, embed_size, inner_ff_size, dropout)]\n",
        "        self.encoders = nn.ModuleList(encoders)                   # Holds list of encoder heads\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_size)                      # ToDo: Determine direction of normalization\n",
        "        self.linear = nn.Linear(embed_size, n_embeddings, bias=False) # In this example specifically [128, 40000]\n",
        "\n",
        "\n",
        "    def forward(self, x):                                         # x is a batch of vectors, vector contains word IDs; [128, 20, 1]\n",
        "        x = self.embeddings(x)                                    # Word IDs converted to embeddings; [128, 20, 128]\n",
        "        x = x + self.pe(x)                                        # Add positional encoding to 3d matrix from line above, eg: [batch, word, embedding + pos_enc]\n",
        "        for encoder in self.encoders:                             # Iterate through list of Multi-Attention Heads\n",
        "            x = encoder(x)                                        # ToDo: Determine shape\n",
        "        x = self.norm(x)                                            # Applies normalization to the last dimension of the tensor; [batch, word, normalized vector representation]\n",
        "        x = self.linear(x)                                        # Applies linear layer to each word embedding; [128, 20, 40000]\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "DYUOS5XUcbZu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Create Model"
      ],
      "metadata": {
        "id": "B_3w5wwVcj84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the previously defined Transformer class and push to GPU\n",
        "print('initializing model...')\n",
        "model = Transformer(n_encoders, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n",
        "model = model.cuda()\n",
        "\n",
        "# Create Adam optimizer and CrossEntropyLoss objects\n",
        "print('initializing optimizer and loss...')\n",
        "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
        "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)   # ignore_index used to mask entries not contributing to loss"
      ],
      "metadata": {
        "id": "BVGEJNuLcnIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1007cb5a-e549-4ed2-e04c-ac09f9447ea1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing model...\n",
            "initializing optimizer and loss...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Train Model"
      ],
      "metadata": {
        "id": "9DTFMab-csF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training...\")\n",
        "model.train()\n",
        "batch_iter = iter(data_loader)\n",
        "\n",
        "for it in range(n_iteration):\n",
        "    batch, batch_iter = get_batch(data_loader, batch_iter)  # Get batch\n",
        "\n",
        "    masked_input = batch[\"input\"]                           # Infer\n",
        "    masked_target = batch[\"target\"]                         # Infer\n",
        "\n",
        "    masked_input = masked_input.cuda(non_blocking=True)\n",
        "    masked_target = masked_target.cuda(non_blocking=True)\n",
        "\n",
        "    output = model(masked_input)\n",
        "\n",
        "    output_v = output.view(-1,output.shape[-1])\n",
        "    target_v = masked_target.view(-1,1).squeeze()\n",
        "    loss = loss_model(output_v, target_v)                   # Compute the cross entropy loss\n",
        "\n",
        "    loss.backward()                                         # Compute gradients\n",
        "    optimizer.step()                                        # Apply gradients\n",
        "\n",
        "    if it % print_each == 0:\n",
        "        print(\"Iteration:\", it,\n",
        "              \" | Loss\", np.round(loss.item(),2),\n",
        "              \" | Δw:\", round(model.embeddings.weight.grad.abs().sum().item(),3))\n",
        "\n",
        "    optimizer.zero_grad()                                   # Reset gradients"
      ],
      "metadata": {
        "id": "buYOajGZdduO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b77b57a7-79fc-4167-f89a-c29eea26ddb9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n",
            "Iteration: 0  | Loss 10.31  | Δw: 1.6\n",
            "Iteration: 5  | Loss 8.8  | Δw: 0.125\n",
            "Iteration: 10  | Loss 7.74  | Δw: 0.064\n",
            "Iteration: 15  | Loss 6.68  | Δw: 0.052\n",
            "Iteration: 20  | Loss 6.78  | Δw: 0.042\n",
            "Iteration: 25  | Loss 6.71  | Δw: 0.044\n",
            "Iteration: 30  | Loss 6.68  | Δw: 0.042\n",
            "Iteration: 35  | Loss 6.57  | Δw: 0.045\n",
            "Iteration: 40  | Loss 7.04  | Δw: 0.051\n",
            "Iteration: 45  | Loss 6.38  | Δw: 0.045\n",
            "Iteration: 50  | Loss 6.45  | Δw: 0.035\n",
            "Iteration: 55  | Loss 6.79  | Δw: 0.041\n",
            "Iteration: 60  | Loss 6.76  | Δw: 0.038\n",
            "Iteration: 65  | Loss 6.46  | Δw: 0.037\n",
            "Iteration: 70  | Loss 6.31  | Δw: 0.039\n",
            "Iteration: 75  | Loss 6.75  | Δw: 0.039\n",
            "Iteration: 80  | Loss 6.34  | Δw: 0.038\n",
            "Iteration: 85  | Loss 6.26  | Δw: 0.041\n",
            "Iteration: 90  | Loss 6.24  | Δw: 0.05\n",
            "Iteration: 95  | Loss 6.39  | Δw: 0.061\n",
            "Iteration: 100  | Loss 6.28  | Δw: 0.049\n",
            "Iteration: 105  | Loss 6.62  | Δw: 0.068\n",
            "Iteration: 110  | Loss 6.6  | Δw: 0.086\n",
            "Iteration: 115  | Loss 6.58  | Δw: 0.093\n",
            "Iteration: 120  | Loss 6.28  | Δw: 0.11\n",
            "Iteration: 125  | Loss 6.44  | Δw: 0.174\n",
            "Iteration: 130  | Loss 6.34  | Δw: 0.196\n",
            "Iteration: 135  | Loss 6.69  | Δw: 0.174\n",
            "Iteration: 140  | Loss 6.45  | Δw: 0.339\n",
            "Iteration: 145  | Loss 6.57  | Δw: 0.285\n",
            "Iteration: 150  | Loss 6.6  | Δw: 0.362\n",
            "Iteration: 155  | Loss 6.51  | Δw: 0.347\n",
            "Iteration: 160  | Loss 6.42  | Δw: 0.422\n",
            "Iteration: 165  | Loss 6.46  | Δw: 0.468\n",
            "Iteration: 170  | Loss 6.2  | Δw: 0.459\n",
            "Iteration: 175  | Loss 6.36  | Δw: 0.417\n",
            "Iteration: 180  | Loss 6.29  | Δw: 0.429\n",
            "Iteration: 185  | Loss 6.41  | Δw: 0.4\n",
            "Iteration: 190  | Loss 6.28  | Δw: 0.551\n",
            "Iteration: 195  | Loss 6.31  | Δw: 0.648\n",
            "Iteration: 200  | Loss 6.38  | Δw: 0.791\n",
            "Iteration: 205  | Loss 6.39  | Δw: 0.752\n",
            "Iteration: 210  | Loss 6.26  | Δw: 1.054\n",
            "Iteration: 215  | Loss 6.46  | Δw: 0.907\n",
            "Iteration: 220  | Loss 6.44  | Δw: 1.42\n",
            "Iteration: 225  | Loss 6.29  | Δw: 0.942\n",
            "Iteration: 230  | Loss 6.55  | Δw: 0.926\n",
            "Iteration: 235  | Loss 6.44  | Δw: 1.344\n",
            "Iteration: 240  | Loss 6.25  | Δw: 0.888\n",
            "Iteration: 245  | Loss 5.98  | Δw: 1.101\n",
            "Iteration: 250  | Loss 6.48  | Δw: 1.179\n",
            "Iteration: 255  | Loss 6.26  | Δw: 1.78\n",
            "Iteration: 260  | Loss 5.96  | Δw: 1.088\n",
            "Iteration: 265  | Loss 6.27  | Δw: 1.398\n",
            "Iteration: 270  | Loss 6.08  | Δw: 1.92\n",
            "Iteration: 275  | Loss 6.28  | Δw: 2.532\n",
            "Iteration: 280  | Loss 6.06  | Δw: 1.9\n",
            "Iteration: 285  | Loss 6.14  | Δw: 2.072\n",
            "Iteration: 290  | Loss 6.1  | Δw: 1.744\n",
            "Iteration: 295  | Loss 6.17  | Δw: 2.368\n",
            "Iteration: 300  | Loss 5.97  | Δw: 2.323\n",
            "Iteration: 305  | Loss 6.13  | Δw: 1.861\n",
            "Iteration: 310  | Loss 6.22  | Δw: 2.394\n",
            "Iteration: 315  | Loss 5.93  | Δw: 2.528\n",
            "Iteration: 320  | Loss 6.24  | Δw: 3.359\n",
            "Iteration: 325  | Loss 6.06  | Δw: 3.41\n",
            "Iteration: 330  | Loss 6.06  | Δw: 3.016\n",
            "Iteration: 335  | Loss 5.92  | Δw: 2.149\n",
            "Iteration: 340  | Loss 5.9  | Δw: 2.573\n",
            "Iteration: 345  | Loss 5.68  | Δw: 3.366\n",
            "Iteration: 350  | Loss 6.02  | Δw: 3.645\n",
            "Iteration: 355  | Loss 6.2  | Δw: 2.915\n",
            "Iteration: 360  | Loss 5.86  | Δw: 3.822\n",
            "Iteration: 365  | Loss 6.06  | Δw: 3.64\n",
            "Iteration: 370  | Loss 6.07  | Δw: 4.318\n",
            "Iteration: 375  | Loss 6.32  | Δw: 3.519\n",
            "Iteration: 380  | Loss 5.99  | Δw: 4.584\n",
            "Iteration: 385  | Loss 6.07  | Δw: 3.676\n",
            "Iteration: 390  | Loss 6.02  | Δw: 3.987\n",
            "Iteration: 395  | Loss 5.9  | Δw: 4.02\n",
            "Iteration: 400  | Loss 5.82  | Δw: 4.521\n",
            "Iteration: 405  | Loss 5.98  | Δw: 4.377\n",
            "Iteration: 410  | Loss 5.98  | Δw: 4.952\n",
            "Iteration: 415  | Loss 5.81  | Δw: 6.516\n",
            "Iteration: 420  | Loss 5.77  | Δw: 6.799\n",
            "Iteration: 425  | Loss 6.0  | Δw: 5.984\n",
            "Iteration: 430  | Loss 5.83  | Δw: 6.031\n",
            "Iteration: 435  | Loss 5.87  | Δw: 5.224\n",
            "Iteration: 440  | Loss 6.06  | Δw: 7.238\n",
            "Iteration: 445  | Loss 5.82  | Δw: 6.425\n",
            "Iteration: 450  | Loss 5.78  | Δw: 6.719\n",
            "Iteration: 455  | Loss 5.77  | Δw: 6.196\n",
            "Iteration: 460  | Loss 5.89  | Δw: 5.951\n",
            "Iteration: 465  | Loss 5.48  | Δw: 5.574\n",
            "Iteration: 470  | Loss 5.93  | Δw: 7.168\n",
            "Iteration: 475  | Loss 5.75  | Δw: 7.804\n",
            "Iteration: 480  | Loss 5.78  | Δw: 7.741\n",
            "Iteration: 485  | Loss 5.62  | Δw: 6.927\n",
            "Iteration: 490  | Loss 5.67  | Δw: 7.621\n",
            "Iteration: 495  | Loss 5.77  | Δw: 7.609\n",
            "Iteration: 500  | Loss 5.56  | Δw: 8.875\n",
            "Iteration: 505  | Loss 5.79  | Δw: 7.852\n",
            "Iteration: 510  | Loss 5.32  | Δw: 8.862\n",
            "Iteration: 515  | Loss 5.72  | Δw: 8.906\n",
            "Iteration: 520  | Loss 5.53  | Δw: 8.154\n",
            "Iteration: 525  | Loss 5.42  | Δw: 8.297\n",
            "Iteration: 530  | Loss 5.75  | Δw: 8.315\n",
            "Iteration: 535  | Loss 5.81  | Δw: 9.643\n",
            "Iteration: 540  | Loss 5.75  | Δw: 8.574\n",
            "Iteration: 545  | Loss 5.63  | Δw: 11.17\n",
            "Iteration: 550  | Loss 5.54  | Δw: 10.099\n",
            "Iteration: 555  | Loss 5.44  | Δw: 11.363\n",
            "Iteration: 560  | Loss 5.52  | Δw: 9.768\n",
            "Iteration: 565  | Loss 5.28  | Δw: 11.388\n",
            "Iteration: 570  | Loss 5.27  | Δw: 10.78\n",
            "Iteration: 575  | Loss 5.57  | Δw: 11.132\n",
            "Iteration: 580  | Loss 5.48  | Δw: 10.063\n",
            "Iteration: 585  | Loss 5.3  | Δw: 10.275\n",
            "Iteration: 590  | Loss 5.24  | Δw: 10.972\n",
            "Iteration: 595  | Loss 5.22  | Δw: 11.032\n",
            "Iteration: 600  | Loss 5.23  | Δw: 12.925\n",
            "Iteration: 605  | Loss 5.32  | Δw: 10.664\n",
            "Iteration: 610  | Loss 5.37  | Δw: 12.695\n",
            "Iteration: 615  | Loss 5.54  | Δw: 11.126\n",
            "Iteration: 620  | Loss 5.31  | Δw: 11.227\n",
            "Iteration: 625  | Loss 5.01  | Δw: 11.197\n",
            "Iteration: 630  | Loss 5.09  | Δw: 12.684\n",
            "Iteration: 635  | Loss 5.38  | Δw: 13.696\n",
            "Iteration: 640  | Loss 5.36  | Δw: 12.74\n",
            "Iteration: 645  | Loss 5.15  | Δw: 13.937\n",
            "Iteration: 650  | Loss 5.4  | Δw: 11.718\n",
            "Iteration: 655  | Loss 5.34  | Δw: 13.925\n",
            "Iteration: 660  | Loss 5.34  | Δw: 14.147\n",
            "Iteration: 665  | Loss 5.15  | Δw: 13.336\n",
            "Iteration: 670  | Loss 5.09  | Δw: 14.278\n",
            "Iteration: 675  | Loss 5.42  | Δw: 12.958\n",
            "Iteration: 680  | Loss 5.39  | Δw: 13.554\n",
            "Iteration: 685  | Loss 5.35  | Δw: 14.863\n",
            "Iteration: 690  | Loss 4.98  | Δw: 14.712\n",
            "Iteration: 695  | Loss 4.98  | Δw: 15.887\n",
            "Iteration: 700  | Loss 5.22  | Δw: 14.116\n",
            "Iteration: 705  | Loss 4.95  | Δw: 14.468\n",
            "Iteration: 710  | Loss 5.18  | Δw: 15.06\n",
            "Iteration: 715  | Loss 5.01  | Δw: 15.973\n",
            "Iteration: 720  | Loss 5.16  | Δw: 16.074\n",
            "Iteration: 725  | Loss 5.18  | Δw: 15.027\n",
            "Iteration: 730  | Loss 5.08  | Δw: 16.407\n",
            "Iteration: 735  | Loss 5.06  | Δw: 15.243\n",
            "Iteration: 740  | Loss 4.75  | Δw: 17.482\n",
            "Iteration: 745  | Loss 4.97  | Δw: 15.118\n",
            "Iteration: 750  | Loss 5.19  | Δw: 17.08\n",
            "Iteration: 755  | Loss 5.03  | Δw: 15.794\n",
            "Iteration: 760  | Loss 4.76  | Δw: 17.13\n",
            "Iteration: 765  | Loss 4.91  | Δw: 17.991\n",
            "Iteration: 770  | Loss 5.0  | Δw: 16.06\n",
            "Iteration: 775  | Loss 5.07  | Δw: 17.885\n",
            "Iteration: 780  | Loss 4.76  | Δw: 18.785\n",
            "Iteration: 785  | Loss 4.88  | Δw: 16.283\n",
            "Iteration: 790  | Loss 4.73  | Δw: 16.387\n",
            "Iteration: 795  | Loss 5.03  | Δw: 19.18\n",
            "Iteration: 800  | Loss 5.05  | Δw: 17.933\n",
            "Iteration: 805  | Loss 4.89  | Δw: 18.642\n",
            "Iteration: 810  | Loss 4.66  | Δw: 17.483\n",
            "Iteration: 815  | Loss 5.08  | Δw: 19.376\n",
            "Iteration: 820  | Loss 4.84  | Δw: 18.563\n",
            "Iteration: 825  | Loss 4.8  | Δw: 16.995\n",
            "Iteration: 830  | Loss 4.91  | Δw: 19.299\n",
            "Iteration: 835  | Loss 4.7  | Δw: 18.475\n",
            "Iteration: 840  | Loss 4.99  | Δw: 18.278\n",
            "Iteration: 845  | Loss 5.19  | Δw: 19.44\n",
            "Iteration: 850  | Loss 4.85  | Δw: 18.588\n",
            "Iteration: 855  | Loss 4.89  | Δw: 19.551\n",
            "Iteration: 860  | Loss 4.69  | Δw: 19.046\n",
            "Iteration: 865  | Loss 4.94  | Δw: 17.868\n",
            "Iteration: 870  | Loss 4.65  | Δw: 18.921\n",
            "Iteration: 875  | Loss 4.58  | Δw: 18.5\n",
            "Iteration: 880  | Loss 4.89  | Δw: 19.958\n",
            "Iteration: 885  | Loss 4.82  | Δw: 19.061\n",
            "Iteration: 890  | Loss 4.67  | Δw: 19.483\n",
            "Iteration: 895  | Loss 4.5  | Δw: 21.404\n",
            "Iteration: 900  | Loss 4.66  | Δw: 17.805\n",
            "Iteration: 905  | Loss 4.29  | Δw: 20.067\n",
            "Iteration: 910  | Loss 4.74  | Δw: 18.434\n",
            "Iteration: 915  | Loss 4.71  | Δw: 21.42\n",
            "Iteration: 920  | Loss 4.62  | Δw: 19.406\n",
            "Iteration: 925  | Loss 4.56  | Δw: 18.842\n",
            "Iteration: 930  | Loss 4.72  | Δw: 20.818\n",
            "Iteration: 935  | Loss 4.32  | Δw: 19.516\n",
            "Iteration: 940  | Loss 4.71  | Δw: 20.114\n",
            "Iteration: 945  | Loss 4.55  | Δw: 22.468\n",
            "Iteration: 950  | Loss 4.87  | Δw: 21.922\n",
            "Iteration: 955  | Loss 4.34  | Δw: 21.154\n",
            "Iteration: 960  | Loss 4.66  | Δw: 21.226\n",
            "Iteration: 965  | Loss 4.67  | Δw: 20.508\n",
            "Iteration: 970  | Loss 4.65  | Δw: 22.02\n",
            "Iteration: 975  | Loss 4.66  | Δw: 20.826\n",
            "Iteration: 980  | Loss 4.7  | Δw: 22.05\n",
            "Iteration: 985  | Loss 4.49  | Δw: 21.897\n",
            "Iteration: 990  | Loss 4.55  | Δw: 21.411\n",
            "Iteration: 995  | Loss 4.56  | Δw: 22.766\n",
            "Iteration: 1000  | Loss 4.54  | Δw: 21.39\n",
            "Iteration: 1005  | Loss 4.57  | Δw: 22.446\n",
            "Iteration: 1010  | Loss 4.87  | Δw: 22.796\n",
            "Iteration: 1015  | Loss 4.77  | Δw: 22.198\n",
            "Iteration: 1020  | Loss 4.42  | Δw: 22.058\n",
            "Iteration: 1025  | Loss 4.56  | Δw: 22.577\n",
            "Iteration: 1030  | Loss 4.65  | Δw: 22.41\n",
            "Iteration: 1035  | Loss 4.31  | Δw: 21.532\n",
            "Iteration: 1040  | Loss 4.59  | Δw: 22.367\n",
            "Iteration: 1045  | Loss 4.11  | Δw: 23.327\n",
            "Iteration: 1050  | Loss 4.24  | Δw: 22.723\n",
            "Iteration: 1055  | Loss 4.19  | Δw: 23.193\n",
            "Iteration: 1060  | Loss 4.55  | Δw: 23.58\n",
            "Iteration: 1065  | Loss 4.38  | Δw: 25.344\n",
            "Iteration: 1070  | Loss 4.19  | Δw: 22.348\n",
            "Iteration: 1075  | Loss 4.43  | Δw: 23.171\n",
            "Iteration: 1080  | Loss 4.68  | Δw: 22.118\n",
            "Iteration: 1085  | Loss 4.35  | Δw: 24.397\n",
            "Iteration: 1090  | Loss 4.92  | Δw: 24.271\n",
            "Iteration: 1095  | Loss 4.51  | Δw: 24.521\n",
            "Iteration: 1100  | Loss 4.29  | Δw: 24.113\n",
            "Iteration: 1105  | Loss 4.44  | Δw: 25.667\n",
            "Iteration: 1110  | Loss 4.49  | Δw: 22.553\n",
            "Iteration: 1115  | Loss 4.12  | Δw: 23.614\n",
            "Iteration: 1120  | Loss 4.21  | Δw: 23.753\n",
            "Iteration: 1125  | Loss 4.32  | Δw: 23.795\n",
            "Iteration: 1130  | Loss 4.1  | Δw: 23.549\n",
            "Iteration: 1135  | Loss 4.22  | Δw: 22.696\n",
            "Iteration: 1140  | Loss 4.16  | Δw: 26.196\n",
            "Iteration: 1145  | Loss 4.39  | Δw: 23.821\n",
            "Iteration: 1150  | Loss 4.35  | Δw: 23.306\n",
            "Iteration: 1155  | Loss 4.46  | Δw: 27.294\n",
            "Iteration: 1160  | Loss 4.43  | Δw: 25.152\n",
            "Iteration: 1165  | Loss 4.14  | Δw: 24.212\n",
            "Iteration: 1170  | Loss 4.53  | Δw: 22.694\n",
            "Iteration: 1175  | Loss 4.51  | Δw: 25.629\n",
            "Iteration: 1180  | Loss 4.5  | Δw: 26.32\n",
            "Iteration: 1185  | Loss 4.18  | Δw: 25.5\n",
            "Iteration: 1190  | Loss 3.96  | Δw: 26.848\n",
            "Iteration: 1195  | Loss 4.32  | Δw: 26.002\n",
            "Iteration: 1200  | Loss 4.31  | Δw: 23.763\n",
            "Iteration: 1205  | Loss 4.3  | Δw: 27.266\n",
            "Iteration: 1210  | Loss 4.07  | Δw: 26.112\n",
            "Iteration: 1215  | Loss 3.97  | Δw: 25.193\n",
            "Iteration: 1220  | Loss 4.14  | Δw: 24.858\n",
            "Iteration: 1225  | Loss 4.12  | Δw: 26.638\n",
            "Iteration: 1230  | Loss 4.29  | Δw: 26.992\n",
            "Iteration: 1235  | Loss 4.42  | Δw: 28.414\n",
            "Iteration: 1240  | Loss 4.02  | Δw: 28.88\n",
            "Iteration: 1245  | Loss 4.04  | Δw: 26.011\n",
            "Iteration: 1250  | Loss 4.29  | Δw: 28.107\n",
            "Iteration: 1255  | Loss 4.43  | Δw: 25.531\n",
            "Iteration: 1260  | Loss 4.13  | Δw: 28.509\n",
            "Iteration: 1265  | Loss 4.08  | Δw: 28.328\n",
            "Iteration: 1270  | Loss 4.26  | Δw: 28.272\n",
            "Iteration: 1275  | Loss 4.11  | Δw: 27.352\n",
            "Iteration: 1280  | Loss 3.99  | Δw: 27.441\n",
            "Iteration: 1285  | Loss 4.07  | Δw: 26.096\n",
            "Iteration: 1290  | Loss 4.4  | Δw: 26.888\n",
            "Iteration: 1295  | Loss 4.01  | Δw: 28.921\n",
            "Iteration: 1300  | Loss 3.94  | Δw: 24.788\n",
            "Iteration: 1305  | Loss 4.32  | Δw: 29.164\n",
            "Iteration: 1310  | Loss 4.46  | Δw: 25.55\n",
            "Iteration: 1315  | Loss 4.17  | Δw: 29.599\n",
            "Iteration: 1320  | Loss 4.07  | Δw: 26.507\n",
            "Iteration: 1325  | Loss 4.27  | Δw: 28.432\n",
            "Iteration: 1330  | Loss 4.25  | Δw: 27.615\n",
            "Iteration: 1335  | Loss 4.12  | Δw: 28.329\n",
            "Iteration: 1340  | Loss 4.27  | Δw: 28.283\n",
            "Iteration: 1345  | Loss 3.81  | Δw: 27.932\n",
            "Iteration: 1350  | Loss 4.36  | Δw: 29.705\n",
            "Iteration: 1355  | Loss 3.92  | Δw: 27.646\n",
            "Iteration: 1360  | Loss 4.09  | Δw: 27.364\n",
            "Iteration: 1365  | Loss 4.03  | Δw: 27.667\n",
            "Iteration: 1370  | Loss 4.25  | Δw: 27.098\n",
            "Iteration: 1375  | Loss 4.39  | Δw: 27.42\n",
            "Iteration: 1380  | Loss 4.23  | Δw: 28.235\n",
            "Iteration: 1385  | Loss 4.12  | Δw: 28.704\n",
            "Iteration: 1390  | Loss 4.7  | Δw: 27.302\n",
            "Iteration: 1395  | Loss 4.65  | Δw: 29.545\n",
            "Iteration: 1400  | Loss 4.08  | Δw: 27.971\n",
            "Iteration: 1405  | Loss 4.02  | Δw: 27.857\n",
            "Iteration: 1410  | Loss 4.3  | Δw: 30.089\n",
            "Iteration: 1415  | Loss 4.08  | Δw: 28.414\n",
            "Iteration: 1420  | Loss 4.23  | Δw: 28.914\n",
            "Iteration: 1425  | Loss 3.8  | Δw: 27.96\n",
            "Iteration: 1430  | Loss 4.1  | Δw: 30.503\n",
            "Iteration: 1435  | Loss 4.13  | Δw: 29.62\n",
            "Iteration: 1440  | Loss 3.88  | Δw: 30.06\n",
            "Iteration: 1445  | Loss 4.32  | Δw: 29.95\n",
            "Iteration: 1450  | Loss 4.1  | Δw: 30.789\n",
            "Iteration: 1455  | Loss 3.85  | Δw: 27.518\n",
            "Iteration: 1460  | Loss 4.16  | Δw: 28.528\n",
            "Iteration: 1465  | Loss 3.81  | Δw: 30.775\n",
            "Iteration: 1470  | Loss 3.79  | Δw: 28.469\n",
            "Iteration: 1475  | Loss 4.09  | Δw: 30.511\n",
            "Iteration: 1480  | Loss 3.59  | Δw: 27.516\n",
            "Iteration: 1485  | Loss 4.08  | Δw: 31.381\n",
            "Iteration: 1490  | Loss 4.01  | Δw: 28.197\n",
            "Iteration: 1495  | Loss 4.22  | Δw: 30.99\n",
            "Iteration: 1500  | Loss 4.27  | Δw: 28.91\n",
            "Iteration: 1505  | Loss 4.03  | Δw: 32.424\n",
            "Iteration: 1510  | Loss 4.03  | Δw: 28.517\n",
            "Iteration: 1515  | Loss 3.96  | Δw: 34.622\n",
            "Iteration: 1520  | Loss 3.93  | Δw: 31.236\n",
            "Iteration: 1525  | Loss 4.03  | Δw: 32.609\n",
            "Iteration: 1530  | Loss 3.84  | Δw: 30.678\n",
            "Iteration: 1535  | Loss 4.11  | Δw: 30.527\n",
            "Iteration: 1540  | Loss 4.39  | Δw: 30.696\n",
            "Iteration: 1545  | Loss 4.42  | Δw: 32.744\n",
            "Iteration: 1550  | Loss 3.77  | Δw: 28.707\n",
            "Iteration: 1555  | Loss 4.07  | Δw: 29.141\n",
            "Iteration: 1560  | Loss 3.65  | Δw: 29.008\n",
            "Iteration: 1565  | Loss 4.21  | Δw: 28.945\n",
            "Iteration: 1570  | Loss 3.89  | Δw: 28.384\n",
            "Iteration: 1575  | Loss 4.06  | Δw: 31.4\n",
            "Iteration: 1580  | Loss 3.79  | Δw: 31.766\n",
            "Iteration: 1585  | Loss 4.24  | Δw: 28.655\n",
            "Iteration: 1590  | Loss 3.75  | Δw: 29.536\n",
            "Iteration: 1595  | Loss 4.06  | Δw: 30.552\n",
            "Iteration: 1600  | Loss 4.18  | Δw: 29.07\n",
            "Iteration: 1605  | Loss 3.82  | Δw: 30.424\n",
            "Iteration: 1610  | Loss 3.79  | Δw: 29.178\n",
            "Iteration: 1615  | Loss 4.06  | Δw: 32.955\n",
            "Iteration: 1620  | Loss 4.11  | Δw: 32.426\n",
            "Iteration: 1625  | Loss 3.83  | Δw: 29.912\n",
            "Iteration: 1630  | Loss 4.32  | Δw: 31.439\n",
            "Iteration: 1635  | Loss 3.77  | Δw: 29.169\n",
            "Iteration: 1640  | Loss 3.99  | Δw: 34.201\n",
            "Iteration: 1645  | Loss 3.81  | Δw: 29.291\n",
            "Iteration: 1650  | Loss 3.96  | Δw: 32.801\n",
            "Iteration: 1655  | Loss 4.11  | Δw: 32.984\n",
            "Iteration: 1660  | Loss 3.76  | Δw: 31.696\n",
            "Iteration: 1665  | Loss 3.92  | Δw: 31.755\n",
            "Iteration: 1670  | Loss 4.06  | Δw: 33.701\n",
            "Iteration: 1675  | Loss 3.69  | Δw: 30.734\n",
            "Iteration: 1680  | Loss 3.95  | Δw: 34.057\n",
            "Iteration: 1685  | Loss 3.76  | Δw: 30.861\n",
            "Iteration: 1690  | Loss 3.98  | Δw: 33.117\n",
            "Iteration: 1695  | Loss 3.43  | Δw: 36.478\n",
            "Iteration: 1700  | Loss 3.9  | Δw: 29.134\n",
            "Iteration: 1705  | Loss 3.88  | Δw: 33.489\n",
            "Iteration: 1710  | Loss 3.57  | Δw: 31.987\n",
            "Iteration: 1715  | Loss 3.99  | Δw: 34.58\n",
            "Iteration: 1720  | Loss 4.16  | Δw: 34.585\n",
            "Iteration: 1725  | Loss 4.13  | Δw: 37.455\n",
            "Iteration: 1730  | Loss 3.94  | Δw: 32.537\n",
            "Iteration: 1735  | Loss 3.51  | Δw: 33.354\n",
            "Iteration: 1740  | Loss 3.59  | Δw: 31.886\n",
            "Iteration: 1745  | Loss 4.03  | Δw: 32.806\n",
            "Iteration: 1750  | Loss 3.84  | Δw: 33.752\n",
            "Iteration: 1755  | Loss 4.08  | Δw: 33.502\n",
            "Iteration: 1760  | Loss 3.89  | Δw: 32.582\n",
            "Iteration: 1765  | Loss 3.95  | Δw: 31.505\n",
            "Iteration: 1770  | Loss 3.46  | Δw: 30.005\n",
            "Iteration: 1775  | Loss 4.04  | Δw: 33.478\n",
            "Iteration: 1780  | Loss 4.02  | Δw: 32.992\n",
            "Iteration: 1785  | Loss 3.83  | Δw: 34.302\n",
            "Iteration: 1790  | Loss 3.81  | Δw: 36.701\n",
            "Iteration: 1795  | Loss 3.61  | Δw: 35.646\n",
            "Iteration: 1800  | Loss 3.96  | Δw: 33.838\n",
            "Iteration: 1805  | Loss 3.76  | Δw: 31.683\n",
            "Iteration: 1810  | Loss 3.9  | Δw: 39.426\n",
            "Iteration: 1815  | Loss 3.66  | Δw: 30.601\n",
            "Iteration: 1820  | Loss 3.84  | Δw: 32.768\n",
            "Iteration: 1825  | Loss 3.73  | Δw: 31.871\n",
            "Iteration: 1830  | Loss 4.09  | Δw: 35.967\n",
            "Iteration: 1835  | Loss 4.14  | Δw: 37.33\n",
            "Iteration: 1840  | Loss 3.62  | Δw: 34.007\n",
            "Iteration: 1845  | Loss 3.83  | Δw: 31.232\n",
            "Iteration: 1850  | Loss 3.95  | Δw: 32.767\n",
            "Iteration: 1855  | Loss 3.9  | Δw: 38.964\n",
            "Iteration: 1860  | Loss 3.81  | Δw: 34.478\n",
            "Iteration: 1865  | Loss 4.38  | Δw: 38.716\n",
            "Iteration: 1870  | Loss 4.05  | Δw: 35.727\n",
            "Iteration: 1875  | Loss 3.86  | Δw: 34.561\n",
            "Iteration: 1880  | Loss 3.87  | Δw: 31.498\n",
            "Iteration: 1885  | Loss 3.62  | Δw: 33.614\n",
            "Iteration: 1890  | Loss 3.99  | Δw: 35.509\n",
            "Iteration: 1895  | Loss 3.97  | Δw: 35.494\n",
            "Iteration: 1900  | Loss 3.81  | Δw: 36.327\n",
            "Iteration: 1905  | Loss 3.79  | Δw: 33.113\n",
            "Iteration: 1910  | Loss 3.83  | Δw: 36.749\n",
            "Iteration: 1915  | Loss 3.66  | Δw: 34.858\n",
            "Iteration: 1920  | Loss 3.72  | Δw: 35.061\n",
            "Iteration: 1925  | Loss 3.52  | Δw: 35.393\n",
            "Iteration: 1930  | Loss 3.89  | Δw: 41.676\n",
            "Iteration: 1935  | Loss 3.96  | Δw: 35.688\n",
            "Iteration: 1940  | Loss 3.67  | Δw: 37.096\n",
            "Iteration: 1945  | Loss 3.73  | Δw: 40.117\n",
            "Iteration: 1950  | Loss 3.57  | Δw: 35.389\n",
            "Iteration: 1955  | Loss 3.78  | Δw: 34.939\n",
            "Iteration: 1960  | Loss 3.82  | Δw: 39.222\n",
            "Iteration: 1965  | Loss 3.8  | Δw: 38.412\n",
            "Iteration: 1970  | Loss 3.78  | Δw: 40.358\n",
            "Iteration: 1975  | Loss 3.38  | Δw: 38.242\n",
            "Iteration: 1980  | Loss 3.95  | Δw: 35.443\n",
            "Iteration: 1985  | Loss 3.74  | Δw: 34.975\n",
            "Iteration: 1990  | Loss 3.7  | Δw: 40.431\n",
            "Iteration: 1995  | Loss 3.91  | Δw: 41.999\n",
            "Iteration: 2000  | Loss 4.01  | Δw: 33.347\n",
            "Iteration: 2005  | Loss 3.64  | Δw: 38.528\n",
            "Iteration: 2010  | Loss 3.79  | Δw: 39.025\n",
            "Iteration: 2015  | Loss 4.04  | Δw: 36.134\n",
            "Iteration: 2020  | Loss 3.9  | Δw: 36.762\n",
            "Iteration: 2025  | Loss 3.54  | Δw: 33.932\n",
            "Iteration: 2030  | Loss 4.09  | Δw: 40.356\n",
            "Iteration: 2035  | Loss 3.95  | Δw: 36.183\n",
            "Iteration: 2040  | Loss 3.81  | Δw: 34.221\n",
            "Iteration: 2045  | Loss 3.55  | Δw: 37.41\n",
            "Iteration: 2050  | Loss 3.7  | Δw: 38.838\n",
            "Iteration: 2055  | Loss 4.18  | Δw: 37.056\n",
            "Iteration: 2060  | Loss 3.96  | Δw: 38.391\n",
            "Iteration: 2065  | Loss 3.63  | Δw: 34.368\n",
            "Iteration: 2070  | Loss 3.41  | Δw: 36.74\n",
            "Iteration: 2075  | Loss 3.93  | Δw: 36.968\n",
            "Iteration: 2080  | Loss 3.75  | Δw: 35.723\n",
            "Iteration: 2085  | Loss 3.91  | Δw: 35.397\n",
            "Iteration: 2090  | Loss 3.63  | Δw: 38.688\n",
            "Iteration: 2095  | Loss 3.97  | Δw: 35.813\n",
            "Iteration: 2100  | Loss 3.38  | Δw: 37.634\n",
            "Iteration: 2105  | Loss 3.62  | Δw: 33.625\n",
            "Iteration: 2110  | Loss 3.67  | Δw: 36.419\n",
            "Iteration: 2115  | Loss 3.64  | Δw: 35.918\n",
            "Iteration: 2120  | Loss 3.87  | Δw: 35.681\n",
            "Iteration: 2125  | Loss 3.38  | Δw: 33.049\n",
            "Iteration: 2130  | Loss 3.92  | Δw: 38.595\n",
            "Iteration: 2135  | Loss 3.63  | Δw: 35.951\n",
            "Iteration: 2140  | Loss 3.67  | Δw: 36.417\n",
            "Iteration: 2145  | Loss 3.81  | Δw: 40.462\n",
            "Iteration: 2150  | Loss 4.12  | Δw: 37.904\n",
            "Iteration: 2155  | Loss 3.67  | Δw: 38.466\n",
            "Iteration: 2160  | Loss 3.85  | Δw: 36.587\n",
            "Iteration: 2165  | Loss 3.81  | Δw: 39.147\n",
            "Iteration: 2170  | Loss 4.0  | Δw: 35.645\n",
            "Iteration: 2175  | Loss 3.69  | Δw: 44.157\n",
            "Iteration: 2180  | Loss 4.02  | Δw: 39.163\n",
            "Iteration: 2185  | Loss 3.9  | Δw: 38.133\n",
            "Iteration: 2190  | Loss 3.76  | Δw: 40.633\n",
            "Iteration: 2195  | Loss 4.0  | Δw: 39.256\n",
            "Iteration: 2200  | Loss 4.02  | Δw: 38.161\n",
            "Iteration: 2205  | Loss 3.84  | Δw: 34.781\n",
            "Iteration: 2210  | Loss 3.82  | Δw: 38.907\n",
            "Iteration: 2215  | Loss 3.86  | Δw: 35.752\n",
            "Iteration: 2220  | Loss 3.72  | Δw: 36.711\n",
            "Iteration: 2225  | Loss 3.62  | Δw: 41.502\n",
            "Iteration: 2230  | Loss 3.96  | Δw: 37.235\n",
            "Iteration: 2235  | Loss 3.7  | Δw: 37.831\n",
            "Iteration: 2240  | Loss 3.84  | Δw: 36.389\n",
            "Iteration: 2245  | Loss 3.71  | Δw: 40.726\n",
            "Iteration: 2250  | Loss 3.66  | Δw: 40.014\n",
            "Iteration: 2255  | Loss 3.54  | Δw: 39.25\n",
            "Iteration: 2260  | Loss 3.63  | Δw: 38.93\n",
            "Iteration: 2265  | Loss 3.43  | Δw: 38.611\n",
            "Iteration: 2270  | Loss 3.69  | Δw: 41.93\n",
            "Iteration: 2275  | Loss 3.56  | Δw: 41.095\n",
            "Iteration: 2280  | Loss 3.64  | Δw: 38.074\n",
            "Iteration: 2285  | Loss 3.97  | Δw: 40.372\n",
            "Iteration: 2290  | Loss 4.11  | Δw: 40.229\n",
            "Iteration: 2295  | Loss 3.75  | Δw: 38.515\n",
            "Iteration: 2300  | Loss 3.86  | Δw: 40.91\n",
            "Iteration: 2305  | Loss 4.07  | Δw: 43.719\n",
            "Iteration: 2310  | Loss 3.7  | Δw: 40.202\n",
            "Iteration: 2315  | Loss 3.76  | Δw: 39.998\n",
            "Iteration: 2320  | Loss 3.89  | Δw: 39.92\n",
            "Iteration: 2325  | Loss 3.72  | Δw: 40.112\n",
            "Iteration: 2330  | Loss 3.7  | Δw: 39.925\n",
            "Iteration: 2335  | Loss 3.73  | Δw: 37.384\n",
            "Iteration: 2340  | Loss 3.78  | Δw: 39.314\n",
            "Iteration: 2345  | Loss 3.91  | Δw: 40.544\n",
            "Iteration: 2350  | Loss 3.63  | Δw: 38.318\n",
            "Iteration: 2355  | Loss 3.56  | Δw: 38.926\n",
            "Iteration: 2360  | Loss 3.63  | Δw: 40.497\n",
            "Iteration: 2365  | Loss 3.39  | Δw: 38.63\n",
            "Iteration: 2370  | Loss 3.79  | Δw: 40.131\n",
            "Iteration: 2375  | Loss 3.95  | Δw: 41.18\n",
            "Iteration: 2380  | Loss 3.66  | Δw: 40.62\n",
            "Iteration: 2385  | Loss 3.7  | Δw: 41.4\n",
            "Iteration: 2390  | Loss 3.79  | Δw: 42.033\n",
            "Iteration: 2395  | Loss 3.44  | Δw: 38.407\n",
            "Iteration: 2400  | Loss 3.63  | Δw: 39.367\n",
            "Iteration: 2405  | Loss 3.67  | Δw: 39.454\n",
            "Iteration: 2410  | Loss 3.47  | Δw: 43.352\n",
            "Iteration: 2415  | Loss 3.5  | Δw: 37.36\n",
            "Iteration: 2420  | Loss 3.54  | Δw: 40.233\n",
            "Iteration: 2425  | Loss 3.86  | Δw: 39.555\n",
            "Iteration: 2430  | Loss 3.62  | Δw: 35.827\n",
            "Iteration: 2435  | Loss 3.85  | Δw: 43.257\n",
            "Iteration: 2440  | Loss 3.65  | Δw: 38.615\n",
            "Iteration: 2445  | Loss 3.68  | Δw: 40.767\n",
            "Iteration: 2450  | Loss 3.66  | Δw: 41.454\n",
            "Iteration: 2455  | Loss 3.76  | Δw: 41.329\n",
            "Iteration: 2460  | Loss 3.88  | Δw: 40.823\n",
            "Iteration: 2465  | Loss 3.76  | Δw: 39.785\n",
            "Iteration: 2470  | Loss 3.5  | Δw: 41.94\n",
            "Iteration: 2475  | Loss 3.66  | Δw: 40.983\n",
            "Iteration: 2480  | Loss 3.59  | Δw: 46.798\n",
            "Iteration: 2485  | Loss 3.63  | Δw: 40.173\n",
            "Iteration: 2490  | Loss 3.8  | Δw: 39.408\n",
            "Iteration: 2495  | Loss 3.73  | Δw: 40.86\n",
            "Iteration: 2500  | Loss 3.67  | Δw: 37.112\n",
            "Iteration: 2505  | Loss 3.47  | Δw: 40.609\n",
            "Iteration: 2510  | Loss 3.55  | Δw: 40.034\n",
            "Iteration: 2515  | Loss 3.59  | Δw: 40.442\n",
            "Iteration: 2520  | Loss 3.6  | Δw: 42.657\n",
            "Iteration: 2525  | Loss 3.7  | Δw: 43.159\n",
            "Iteration: 2530  | Loss 3.48  | Δw: 43.398\n",
            "Iteration: 2535  | Loss 3.71  | Δw: 40.632\n",
            "Iteration: 2540  | Loss 3.73  | Δw: 44.618\n",
            "Iteration: 2545  | Loss 3.86  | Δw: 42.526\n",
            "Iteration: 2550  | Loss 3.87  | Δw: 41.553\n",
            "Iteration: 2555  | Loss 3.7  | Δw: 41.737\n",
            "Iteration: 2560  | Loss 3.76  | Δw: 40.122\n",
            "Iteration: 2565  | Loss 3.59  | Δw: 40.777\n",
            "Iteration: 2570  | Loss 3.46  | Δw: 38.825\n",
            "Iteration: 2575  | Loss 3.44  | Δw: 42.221\n",
            "Iteration: 2580  | Loss 3.64  | Δw: 39.195\n",
            "Iteration: 2585  | Loss 3.73  | Δw: 42.933\n",
            "Iteration: 2590  | Loss 3.69  | Δw: 42.111\n",
            "Iteration: 2595  | Loss 3.77  | Δw: 44.122\n",
            "Iteration: 2600  | Loss 3.57  | Δw: 42.494\n",
            "Iteration: 2605  | Loss 3.69  | Δw: 43.212\n",
            "Iteration: 2610  | Loss 3.55  | Δw: 40.137\n",
            "Iteration: 2615  | Loss 3.45  | Δw: 42.689\n",
            "Iteration: 2620  | Loss 3.5  | Δw: 41.444\n",
            "Iteration: 2625  | Loss 3.56  | Δw: 42.34\n",
            "Iteration: 2630  | Loss 3.62  | Δw: 39.747\n",
            "Iteration: 2635  | Loss 3.67  | Δw: 41.114\n",
            "Iteration: 2640  | Loss 3.54  | Δw: 37.771\n",
            "Iteration: 2645  | Loss 3.46  | Δw: 41.432\n",
            "Iteration: 2650  | Loss 3.67  | Δw: 42.447\n",
            "Iteration: 2655  | Loss 3.57  | Δw: 43.513\n",
            "Iteration: 2660  | Loss 3.93  | Δw: 42.285\n",
            "Iteration: 2665  | Loss 3.88  | Δw: 42.442\n",
            "Iteration: 2670  | Loss 3.7  | Δw: 44.825\n",
            "Iteration: 2675  | Loss 3.66  | Δw: 42.287\n",
            "Iteration: 2680  | Loss 3.36  | Δw: 39.749\n",
            "Iteration: 2685  | Loss 3.53  | Δw: 41.96\n",
            "Iteration: 2690  | Loss 3.63  | Δw: 40.582\n",
            "Iteration: 2695  | Loss 3.36  | Δw: 39.986\n",
            "Iteration: 2700  | Loss 3.31  | Δw: 42.684\n",
            "Iteration: 2705  | Loss 3.39  | Δw: 38.114\n",
            "Iteration: 2710  | Loss 3.46  | Δw: 41.84\n",
            "Iteration: 2715  | Loss 3.48  | Δw: 42.016\n",
            "Iteration: 2720  | Loss 3.62  | Δw: 40.5\n",
            "Iteration: 2725  | Loss 3.77  | Δw: 45.373\n",
            "Iteration: 2730  | Loss 3.69  | Δw: 42.371\n",
            "Iteration: 2735  | Loss 3.53  | Δw: 39.002\n",
            "Iteration: 2740  | Loss 3.46  | Δw: 40.514\n",
            "Iteration: 2745  | Loss 3.64  | Δw: 44.161\n",
            "Iteration: 2750  | Loss 3.57  | Δw: 43.023\n",
            "Iteration: 2755  | Loss 3.68  | Δw: 41.144\n",
            "Iteration: 2760  | Loss 3.43  | Δw: 41.568\n",
            "Iteration: 2765  | Loss 3.79  | Δw: 44.448\n",
            "Iteration: 2770  | Loss 3.88  | Δw: 43.264\n",
            "Iteration: 2775  | Loss 3.61  | Δw: 44.2\n",
            "Iteration: 2780  | Loss 3.49  | Δw: 43.681\n",
            "Iteration: 2785  | Loss 3.81  | Δw: 44.109\n",
            "Iteration: 2790  | Loss 3.89  | Δw: 41.083\n",
            "Iteration: 2795  | Loss 3.82  | Δw: 43.079\n",
            "Iteration: 2800  | Loss 3.88  | Δw: 44.639\n",
            "Iteration: 2805  | Loss 3.94  | Δw: 48.293\n",
            "Iteration: 2810  | Loss 3.44  | Δw: 40.645\n",
            "Iteration: 2815  | Loss 3.69  | Δw: 45.434\n",
            "Iteration: 2820  | Loss 3.49  | Δw: 41.931\n",
            "Iteration: 2825  | Loss 3.26  | Δw: 40.204\n",
            "Iteration: 2830  | Loss 3.56  | Δw: 40.943\n",
            "Iteration: 2835  | Loss 3.2  | Δw: 43.299\n",
            "Iteration: 2840  | Loss 3.78  | Δw: 43.508\n",
            "Iteration: 2845  | Loss 3.8  | Δw: 50.305\n",
            "Iteration: 2850  | Loss 3.77  | Δw: 40.105\n",
            "Iteration: 2855  | Loss 3.83  | Δw: 42.932\n",
            "Iteration: 2860  | Loss 3.51  | Δw: 42.3\n",
            "Iteration: 2865  | Loss 3.55  | Δw: 41.392\n",
            "Iteration: 2870  | Loss 3.79  | Δw: 47.584\n",
            "Iteration: 2875  | Loss 3.44  | Δw: 41.201\n",
            "Iteration: 2880  | Loss 3.5  | Δw: 41.729\n",
            "Iteration: 2885  | Loss 3.95  | Δw: 45.601\n",
            "Iteration: 2890  | Loss 3.48  | Δw: 43.523\n",
            "Iteration: 2895  | Loss 3.48  | Δw: 43.884\n",
            "Iteration: 2900  | Loss 3.41  | Δw: 40.197\n",
            "Iteration: 2905  | Loss 3.26  | Δw: 43.262\n",
            "Iteration: 2910  | Loss 3.65  | Δw: 45.718\n",
            "Iteration: 2915  | Loss 3.25  | Δw: 44.832\n",
            "Iteration: 2920  | Loss 3.29  | Δw: 45.59\n",
            "Iteration: 2925  | Loss 3.51  | Δw: 47.354\n",
            "Iteration: 2930  | Loss 3.57  | Δw: 44.552\n",
            "Iteration: 2935  | Loss 3.43  | Δw: 42.737\n",
            "Iteration: 2940  | Loss 3.6  | Δw: 40.619\n",
            "Iteration: 2945  | Loss 3.51  | Δw: 47.375\n",
            "Iteration: 2950  | Loss 3.72  | Δw: 43.186\n",
            "Iteration: 2955  | Loss 3.87  | Δw: 43.522\n",
            "Iteration: 2960  | Loss 3.53  | Δw: 47.389\n",
            "Iteration: 2965  | Loss 3.2  | Δw: 43.749\n",
            "Iteration: 2970  | Loss 3.52  | Δw: 44.022\n",
            "Iteration: 2975  | Loss 3.57  | Δw: 43.858\n",
            "Iteration: 2980  | Loss 3.42  | Δw: 45.639\n",
            "Iteration: 2985  | Loss 3.77  | Δw: 50.263\n",
            "Iteration: 2990  | Loss 3.38  | Δw: 46.981\n",
            "Iteration: 2995  | Loss 3.91  | Δw: 44.471\n",
            "Iteration: 3000  | Loss 3.67  | Δw: 42.014\n",
            "Iteration: 3005  | Loss 3.63  | Δw: 44.175\n",
            "Iteration: 3010  | Loss 3.41  | Δw: 39.598\n",
            "Iteration: 3015  | Loss 3.9  | Δw: 44.862\n",
            "Iteration: 3020  | Loss 3.66  | Δw: 48.141\n",
            "Iteration: 3025  | Loss 3.5  | Δw: 44.11\n",
            "Iteration: 3030  | Loss 3.8  | Δw: 43.591\n",
            "Iteration: 3035  | Loss 3.45  | Δw: 45.261\n",
            "Iteration: 3040  | Loss 3.34  | Δw: 46.388\n",
            "Iteration: 3045  | Loss 3.65  | Δw: 42.825\n",
            "Iteration: 3050  | Loss 3.66  | Δw: 44.588\n",
            "Iteration: 3055  | Loss 3.1  | Δw: 42.088\n",
            "Iteration: 3060  | Loss 3.22  | Δw: 40.366\n",
            "Iteration: 3065  | Loss 3.52  | Δw: 43.185\n",
            "Iteration: 3070  | Loss 3.25  | Δw: 45.488\n",
            "Iteration: 3075  | Loss 3.54  | Δw: 43.207\n",
            "Iteration: 3080  | Loss 3.27  | Δw: 44.744\n",
            "Iteration: 3085  | Loss 3.84  | Δw: 44.225\n",
            "Iteration: 3090  | Loss 3.74  | Δw: 44.168\n",
            "Iteration: 3095  | Loss 3.54  | Δw: 44.649\n",
            "Iteration: 3100  | Loss 3.22  | Δw: 45.926\n",
            "Iteration: 3105  | Loss 3.4  | Δw: 41.616\n",
            "Iteration: 3110  | Loss 3.39  | Δw: 46.4\n",
            "Iteration: 3115  | Loss 3.37  | Δw: 46.079\n",
            "Iteration: 3120  | Loss 3.76  | Δw: 47.535\n",
            "Iteration: 3125  | Loss 3.41  | Δw: 45.635\n",
            "Iteration: 3130  | Loss 3.42  | Δw: 44.063\n",
            "Iteration: 3135  | Loss 3.6  | Δw: 46.14\n",
            "Iteration: 3140  | Loss 3.29  | Δw: 49.355\n",
            "Iteration: 3145  | Loss 3.34  | Δw: 47.9\n",
            "Iteration: 3150  | Loss 3.72  | Δw: 44.219\n",
            "Iteration: 3155  | Loss 3.64  | Δw: 41.475\n",
            "Iteration: 3160  | Loss 3.72  | Δw: 44.904\n",
            "Iteration: 3165  | Loss 3.58  | Δw: 46.139\n",
            "Iteration: 3170  | Loss 3.81  | Δw: 43.306\n",
            "Iteration: 3175  | Loss 3.38  | Δw: 50.98\n",
            "Iteration: 3180  | Loss 3.53  | Δw: 48.508\n",
            "Iteration: 3185  | Loss 3.42  | Δw: 46.122\n",
            "Iteration: 3190  | Loss 3.42  | Δw: 45.002\n",
            "Iteration: 3195  | Loss 3.38  | Δw: 43.648\n",
            "Iteration: 3200  | Loss 3.55  | Δw: 44.538\n",
            "Iteration: 3205  | Loss 3.52  | Δw: 40.754\n",
            "Iteration: 3210  | Loss 3.72  | Δw: 47.38\n",
            "Iteration: 3215  | Loss 3.5  | Δw: 47.334\n",
            "Iteration: 3220  | Loss 3.71  | Δw: 44.504\n",
            "Iteration: 3225  | Loss 3.66  | Δw: 46.506\n",
            "Iteration: 3230  | Loss 3.69  | Δw: 47.429\n",
            "Iteration: 3235  | Loss 3.25  | Δw: 43.689\n",
            "Iteration: 3240  | Loss 3.38  | Δw: 44.797\n",
            "Iteration: 3245  | Loss 3.51  | Δw: 44.2\n",
            "Iteration: 3250  | Loss 3.67  | Δw: 46.316\n",
            "Iteration: 3255  | Loss 3.35  | Δw: 48.323\n",
            "Iteration: 3260  | Loss 3.47  | Δw: 44.059\n",
            "Iteration: 3265  | Loss 3.5  | Δw: 46.412\n",
            "Iteration: 3270  | Loss 3.8  | Δw: 46.394\n",
            "Iteration: 3275  | Loss 3.32  | Δw: 44.234\n",
            "Iteration: 3280  | Loss 3.42  | Δw: 44.024\n",
            "Iteration: 3285  | Loss 3.73  | Δw: 46.473\n",
            "Iteration: 3290  | Loss 3.35  | Δw: 52.422\n",
            "Iteration: 3295  | Loss 3.04  | Δw: 42.158\n",
            "Iteration: 3300  | Loss 3.41  | Δw: 46.516\n",
            "Iteration: 3305  | Loss 3.45  | Δw: 51.726\n",
            "Iteration: 3310  | Loss 3.29  | Δw: 44.682\n",
            "Iteration: 3315  | Loss 3.46  | Δw: 46.695\n",
            "Iteration: 3320  | Loss 3.65  | Δw: 45.708\n",
            "Iteration: 3325  | Loss 3.61  | Δw: 47.29\n",
            "Iteration: 3330  | Loss 3.5  | Δw: 49.666\n",
            "Iteration: 3335  | Loss 3.37  | Δw: 48.308\n",
            "Iteration: 3340  | Loss 3.54  | Δw: 47.403\n",
            "Iteration: 3345  | Loss 3.35  | Δw: 48.488\n",
            "Iteration: 3350  | Loss 3.36  | Δw: 45.042\n",
            "Iteration: 3355  | Loss 3.38  | Δw: 47.662\n",
            "Iteration: 3360  | Loss 3.45  | Δw: 46.572\n",
            "Iteration: 3365  | Loss 3.42  | Δw: 45.377\n",
            "Iteration: 3370  | Loss 3.28  | Δw: 45.29\n",
            "Iteration: 3375  | Loss 3.39  | Δw: 46.603\n",
            "Iteration: 3380  | Loss 3.45  | Δw: 50.2\n",
            "Iteration: 3385  | Loss 3.28  | Δw: 48.43\n",
            "Iteration: 3390  | Loss 3.51  | Δw: 49.821\n",
            "Iteration: 3395  | Loss 3.84  | Δw: 51.772\n",
            "Iteration: 3400  | Loss 3.47  | Δw: 45.543\n",
            "Iteration: 3405  | Loss 3.42  | Δw: 46.572\n",
            "Iteration: 3410  | Loss 3.52  | Δw: 48.351\n",
            "Iteration: 3415  | Loss 3.49  | Δw: 49.412\n",
            "Iteration: 3420  | Loss 3.55  | Δw: 48.715\n",
            "Iteration: 3425  | Loss 3.71  | Δw: 44.67\n",
            "Iteration: 3430  | Loss 3.73  | Δw: 50.6\n",
            "Iteration: 3435  | Loss 3.43  | Δw: 49.128\n",
            "Iteration: 3440  | Loss 3.52  | Δw: 45.242\n",
            "Iteration: 3445  | Loss 3.62  | Δw: 45.76\n",
            "Iteration: 3450  | Loss 3.54  | Δw: 47.668\n",
            "Iteration: 3455  | Loss 3.59  | Δw: 50.876\n",
            "Iteration: 3460  | Loss 3.3  | Δw: 43.282\n",
            "Iteration: 3465  | Loss 3.53  | Δw: 48.819\n",
            "Iteration: 3470  | Loss 3.58  | Δw: 47.189\n",
            "Iteration: 3475  | Loss 3.28  | Δw: 42.715\n",
            "Iteration: 3480  | Loss 3.41  | Δw: 43.259\n",
            "Iteration: 3485  | Loss 3.26  | Δw: 49.101\n",
            "Iteration: 3490  | Loss 3.22  | Δw: 50.597\n",
            "Iteration: 3495  | Loss 3.52  | Δw: 45.318\n",
            "Iteration: 3500  | Loss 3.51  | Δw: 46.007\n",
            "Iteration: 3505  | Loss 3.83  | Δw: 51.553\n",
            "Iteration: 3510  | Loss 3.45  | Δw: 47.031\n",
            "Iteration: 3515  | Loss 3.22  | Δw: 45.532\n",
            "Iteration: 3520  | Loss 3.38  | Δw: 45.501\n",
            "Iteration: 3525  | Loss 3.71  | Δw: 47.632\n",
            "Iteration: 3530  | Loss 3.36  | Δw: 46.733\n",
            "Iteration: 3535  | Loss 3.46  | Δw: 47.857\n",
            "Iteration: 3540  | Loss 3.78  | Δw: 47.774\n",
            "Iteration: 3545  | Loss 3.2  | Δw: 45.863\n",
            "Iteration: 3550  | Loss 3.27  | Δw: 47.199\n",
            "Iteration: 3555  | Loss 3.74  | Δw: 48.755\n",
            "Iteration: 3560  | Loss 3.74  | Δw: 42.407\n",
            "Iteration: 3565  | Loss 3.6  | Δw: 49.244\n",
            "Iteration: 3570  | Loss 3.25  | Δw: 48.336\n",
            "Iteration: 3575  | Loss 3.53  | Δw: 45.681\n",
            "Iteration: 3580  | Loss 3.3  | Δw: 49.938\n",
            "Iteration: 3585  | Loss 3.41  | Δw: 45.271\n",
            "Iteration: 3590  | Loss 3.49  | Δw: 47.338\n",
            "Iteration: 3595  | Loss 3.31  | Δw: 49.737\n",
            "Iteration: 3600  | Loss 3.39  | Δw: 47.518\n",
            "Iteration: 3605  | Loss 3.34  | Δw: 53.866\n",
            "Iteration: 3610  | Loss 3.58  | Δw: 47.293\n",
            "Iteration: 3615  | Loss 3.64  | Δw: 48.426\n",
            "Iteration: 3620  | Loss 3.38  | Δw: 44.739\n",
            "Iteration: 3625  | Loss 3.24  | Δw: 46.429\n",
            "Iteration: 3630  | Loss 3.8  | Δw: 49.084\n",
            "Iteration: 3635  | Loss 3.38  | Δw: 47.162\n",
            "Iteration: 3640  | Loss 3.25  | Δw: 47.192\n",
            "Iteration: 3645  | Loss 3.43  | Δw: 50.188\n",
            "Iteration: 3650  | Loss 3.27  | Δw: 47.074\n",
            "Iteration: 3655  | Loss 3.48  | Δw: 47.042\n",
            "Iteration: 3660  | Loss 3.48  | Δw: 48.665\n",
            "Iteration: 3665  | Loss 3.24  | Δw: 49.271\n",
            "Iteration: 3670  | Loss 3.58  | Δw: 49.287\n",
            "Iteration: 3675  | Loss 3.47  | Δw: 52.486\n",
            "Iteration: 3680  | Loss 3.36  | Δw: 49.899\n",
            "Iteration: 3685  | Loss 3.41  | Δw: 53.107\n",
            "Iteration: 3690  | Loss 3.42  | Δw: 50.343\n",
            "Iteration: 3695  | Loss 3.8  | Δw: 51.704\n",
            "Iteration: 3700  | Loss 3.39  | Δw: 50.819\n",
            "Iteration: 3705  | Loss 3.69  | Δw: 53.299\n",
            "Iteration: 3710  | Loss 3.63  | Δw: 51.763\n",
            "Iteration: 3715  | Loss 3.73  | Δw: 49.591\n",
            "Iteration: 3720  | Loss 3.69  | Δw: 53.815\n",
            "Iteration: 3725  | Loss 3.71  | Δw: 52.52\n",
            "Iteration: 3730  | Loss 3.57  | Δw: 49.98\n",
            "Iteration: 3735  | Loss 3.36  | Δw: 50.732\n",
            "Iteration: 3740  | Loss 3.56  | Δw: 49.488\n",
            "Iteration: 3745  | Loss 3.45  | Δw: 53.207\n",
            "Iteration: 3750  | Loss 3.47  | Δw: 51.143\n",
            "Iteration: 3755  | Loss 3.39  | Δw: 46.877\n",
            "Iteration: 3760  | Loss 3.16  | Δw: 51.213\n",
            "Iteration: 3765  | Loss 3.68  | Δw: 48.114\n",
            "Iteration: 3770  | Loss 3.54  | Δw: 46.949\n",
            "Iteration: 3775  | Loss 3.24  | Δw: 47.528\n",
            "Iteration: 3780  | Loss 3.42  | Δw: 52.211\n",
            "Iteration: 3785  | Loss 3.33  | Δw: 47.338\n",
            "Iteration: 3790  | Loss 3.43  | Δw: 49.772\n",
            "Iteration: 3795  | Loss 3.13  | Δw: 49.799\n",
            "Iteration: 3800  | Loss 2.95  | Δw: 47.811\n",
            "Iteration: 3805  | Loss 3.38  | Δw: 52.985\n",
            "Iteration: 3810  | Loss 3.35  | Δw: 48.366\n",
            "Iteration: 3815  | Loss 3.2  | Δw: 48.846\n",
            "Iteration: 3820  | Loss 3.54  | Δw: 52.589\n",
            "Iteration: 3825  | Loss 3.49  | Δw: 51.328\n",
            "Iteration: 3830  | Loss 3.47  | Δw: 49.236\n",
            "Iteration: 3835  | Loss 3.57  | Δw: 50.238\n",
            "Iteration: 3840  | Loss 3.56  | Δw: 50.964\n",
            "Iteration: 3845  | Loss 3.32  | Δw: 46.687\n",
            "Iteration: 3850  | Loss 3.47  | Δw: 48.461\n",
            "Iteration: 3855  | Loss 3.42  | Δw: 47.863\n",
            "Iteration: 3860  | Loss 3.34  | Δw: 50.839\n",
            "Iteration: 3865  | Loss 2.89  | Δw: 48.068\n",
            "Iteration: 3870  | Loss 3.48  | Δw: 50.356\n",
            "Iteration: 3875  | Loss 3.35  | Δw: 45.987\n",
            "Iteration: 3880  | Loss 3.61  | Δw: 49.225\n",
            "Iteration: 3885  | Loss 3.34  | Δw: 50.913\n",
            "Iteration: 3890  | Loss 3.35  | Δw: 47.313\n",
            "Iteration: 3895  | Loss 3.54  | Δw: 54.454\n",
            "Iteration: 3900  | Loss 3.63  | Δw: 49.544\n",
            "Iteration: 3905  | Loss 3.25  | Δw: 46.457\n",
            "Iteration: 3910  | Loss 3.32  | Δw: 50.426\n",
            "Iteration: 3915  | Loss 3.25  | Δw: 50.491\n",
            "Iteration: 3920  | Loss 3.35  | Δw: 48.275\n",
            "Iteration: 3925  | Loss 3.79  | Δw: 49.921\n",
            "Iteration: 3930  | Loss 3.55  | Δw: 48.647\n",
            "Iteration: 3935  | Loss 3.42  | Δw: 48.436\n",
            "Iteration: 3940  | Loss 3.44  | Δw: 51.994\n",
            "Iteration: 3945  | Loss 3.46  | Δw: 49.951\n",
            "Iteration: 3950  | Loss 3.41  | Δw: 48.843\n",
            "Iteration: 3955  | Loss 3.54  | Δw: 45.348\n",
            "Iteration: 3960  | Loss 3.38  | Δw: 50.064\n",
            "Iteration: 3965  | Loss 3.53  | Δw: 50.384\n",
            "Iteration: 3970  | Loss 3.41  | Δw: 48.954\n",
            "Iteration: 3975  | Loss 3.48  | Δw: 55.739\n",
            "Iteration: 3980  | Loss 3.37  | Δw: 50.55\n",
            "Iteration: 3985  | Loss 3.18  | Δw: 45.958\n",
            "Iteration: 3990  | Loss 3.32  | Δw: 49.795\n",
            "Iteration: 3995  | Loss 3.47  | Δw: 51.254\n",
            "Iteration: 4000  | Loss 3.71  | Δw: 51.615\n",
            "Iteration: 4005  | Loss 3.49  | Δw: 52.368\n",
            "Iteration: 4010  | Loss 3.44  | Δw: 50.93\n",
            "Iteration: 4015  | Loss 3.16  | Δw: 50.936\n",
            "Iteration: 4020  | Loss 3.74  | Δw: 53.683\n",
            "Iteration: 4025  | Loss 3.63  | Δw: 49.803\n",
            "Iteration: 4030  | Loss 3.21  | Δw: 50.73\n",
            "Iteration: 4035  | Loss 3.48  | Δw: 53.776\n",
            "Iteration: 4040  | Loss 3.27  | Δw: 49.952\n",
            "Iteration: 4045  | Loss 3.59  | Δw: 49.199\n",
            "Iteration: 4050  | Loss 3.53  | Δw: 52.774\n",
            "Iteration: 4055  | Loss 3.6  | Δw: 49.473\n",
            "Iteration: 4060  | Loss 3.15  | Δw: 51.722\n",
            "Iteration: 4065  | Loss 3.26  | Δw: 56.429\n",
            "Iteration: 4070  | Loss 3.45  | Δw: 49.729\n",
            "Iteration: 4075  | Loss 3.12  | Δw: 50.081\n",
            "Iteration: 4080  | Loss 3.74  | Δw: 49.653\n",
            "Iteration: 4085  | Loss 3.28  | Δw: 48.592\n",
            "Iteration: 4090  | Loss 3.43  | Δw: 52.613\n",
            "Iteration: 4095  | Loss 3.59  | Δw: 53.463\n",
            "Iteration: 4100  | Loss 3.43  | Δw: 52.657\n",
            "Iteration: 4105  | Loss 3.51  | Δw: 52.835\n",
            "Iteration: 4110  | Loss 3.48  | Δw: 55.559\n",
            "Iteration: 4115  | Loss 3.79  | Δw: 50.916\n",
            "Iteration: 4120  | Loss 3.17  | Δw: 49.561\n",
            "Iteration: 4125  | Loss 3.53  | Δw: 51.096\n",
            "Iteration: 4130  | Loss 3.46  | Δw: 52.112\n",
            "Iteration: 4135  | Loss 3.45  | Δw: 51.06\n",
            "Iteration: 4140  | Loss 3.48  | Δw: 49.97\n",
            "Iteration: 4145  | Loss 3.83  | Δw: 50.935\n",
            "Iteration: 4150  | Loss 3.41  | Δw: 52.931\n",
            "Iteration: 4155  | Loss 3.85  | Δw: 51.306\n",
            "Iteration: 4160  | Loss 3.4  | Δw: 51.518\n",
            "Iteration: 4165  | Loss 3.52  | Δw: 51.502\n",
            "Iteration: 4170  | Loss 3.4  | Δw: 52.001\n",
            "Iteration: 4175  | Loss 3.31  | Δw: 49.156\n",
            "Iteration: 4180  | Loss 3.14  | Δw: 51.618\n",
            "Iteration: 4185  | Loss 3.49  | Δw: 50.939\n",
            "Iteration: 4190  | Loss 3.52  | Δw: 48.628\n",
            "Iteration: 4195  | Loss 3.56  | Δw: 48.992\n",
            "Iteration: 4200  | Loss 3.43  | Δw: 50.524\n",
            "Iteration: 4205  | Loss 3.57  | Δw: 46.816\n",
            "Iteration: 4210  | Loss 3.02  | Δw: 46.176\n",
            "Iteration: 4215  | Loss 3.26  | Δw: 53.997\n",
            "Iteration: 4220  | Loss 3.05  | Δw: 46.37\n",
            "Iteration: 4225  | Loss 3.19  | Δw: 49.297\n",
            "Iteration: 4230  | Loss 3.35  | Δw: 54.623\n",
            "Iteration: 4235  | Loss 3.47  | Δw: 50.011\n",
            "Iteration: 4240  | Loss 3.39  | Δw: 51.954\n",
            "Iteration: 4245  | Loss 3.12  | Δw: 48.727\n",
            "Iteration: 4250  | Loss 3.13  | Δw: 50.761\n",
            "Iteration: 4255  | Loss 3.38  | Δw: 51.289\n",
            "Iteration: 4260  | Loss 3.24  | Δw: 52.64\n",
            "Iteration: 4265  | Loss 3.13  | Δw: 48.975\n",
            "Iteration: 4270  | Loss 3.25  | Δw: 49.924\n",
            "Iteration: 4275  | Loss 3.21  | Δw: 55.022\n",
            "Iteration: 4280  | Loss 3.53  | Δw: 53.238\n",
            "Iteration: 4285  | Loss 3.19  | Δw: 49.458\n",
            "Iteration: 4290  | Loss 3.25  | Δw: 53.979\n",
            "Iteration: 4295  | Loss 3.12  | Δw: 52.057\n",
            "Iteration: 4300  | Loss 3.18  | Δw: 53.887\n",
            "Iteration: 4305  | Loss 3.23  | Δw: 48.99\n",
            "Iteration: 4310  | Loss 3.28  | Δw: 53.101\n",
            "Iteration: 4315  | Loss 3.23  | Δw: 51.026\n",
            "Iteration: 4320  | Loss 3.02  | Δw: 55.891\n",
            "Iteration: 4325  | Loss 3.56  | Δw: 52.807\n",
            "Iteration: 4330  | Loss 3.24  | Δw: 48.66\n",
            "Iteration: 4335  | Loss 3.14  | Δw: 48.636\n",
            "Iteration: 4340  | Loss 3.33  | Δw: 51.676\n",
            "Iteration: 4345  | Loss 3.26  | Δw: 52.942\n",
            "Iteration: 4350  | Loss 3.22  | Δw: 52.941\n",
            "Iteration: 4355  | Loss 3.31  | Δw: 53.905\n",
            "Iteration: 4360  | Loss 3.57  | Δw: 53.685\n",
            "Iteration: 4365  | Loss 3.55  | Δw: 56.699\n",
            "Iteration: 4370  | Loss 3.6  | Δw: 52.554\n",
            "Iteration: 4375  | Loss 3.07  | Δw: 50.789\n",
            "Iteration: 4380  | Loss 3.42  | Δw: 51.793\n",
            "Iteration: 4385  | Loss 3.55  | Δw: 52.05\n",
            "Iteration: 4390  | Loss 3.44  | Δw: 51.187\n",
            "Iteration: 4395  | Loss 3.23  | Δw: 50.395\n",
            "Iteration: 4400  | Loss 3.52  | Δw: 53.27\n",
            "Iteration: 4405  | Loss 3.6  | Δw: 51.085\n",
            "Iteration: 4410  | Loss 3.41  | Δw: 52.203\n",
            "Iteration: 4415  | Loss 3.34  | Δw: 52.043\n",
            "Iteration: 4420  | Loss 3.55  | Δw: 51.359\n",
            "Iteration: 4425  | Loss 3.59  | Δw: 58.577\n",
            "Iteration: 4430  | Loss 3.42  | Δw: 50.064\n",
            "Iteration: 4435  | Loss 3.42  | Δw: 55.952\n",
            "Iteration: 4440  | Loss 3.24  | Δw: 51.576\n",
            "Iteration: 4445  | Loss 3.41  | Δw: 51.349\n",
            "Iteration: 4450  | Loss 3.03  | Δw: 51.703\n",
            "Iteration: 4455  | Loss 3.18  | Δw: 52.3\n",
            "Iteration: 4460  | Loss 2.74  | Δw: 51.339\n",
            "Iteration: 4465  | Loss 3.23  | Δw: 48.412\n",
            "Iteration: 4470  | Loss 3.48  | Δw: 54.045\n",
            "Iteration: 4475  | Loss 3.66  | Δw: 58.824\n",
            "Iteration: 4480  | Loss 3.08  | Δw: 50.577\n",
            "Iteration: 4485  | Loss 3.19  | Δw: 52.44\n",
            "Iteration: 4490  | Loss 3.19  | Δw: 53.005\n",
            "Iteration: 4495  | Loss 3.37  | Δw: 56.926\n",
            "Iteration: 4500  | Loss 3.28  | Δw: 49.98\n",
            "Iteration: 4505  | Loss 3.2  | Δw: 50.179\n",
            "Iteration: 4510  | Loss 3.17  | Δw: 53.971\n",
            "Iteration: 4515  | Loss 3.4  | Δw: 54.385\n",
            "Iteration: 4520  | Loss 3.38  | Δw: 51.841\n",
            "Iteration: 4525  | Loss 3.21  | Δw: 51.835\n",
            "Iteration: 4530  | Loss 3.37  | Δw: 50.254\n",
            "Iteration: 4535  | Loss 3.54  | Δw: 52.633\n",
            "Iteration: 4540  | Loss 3.52  | Δw: 52.579\n",
            "Iteration: 4545  | Loss 2.96  | Δw: 52.797\n",
            "Iteration: 4550  | Loss 3.21  | Δw: 55.325\n",
            "Iteration: 4555  | Loss 3.42  | Δw: 51.609\n",
            "Iteration: 4560  | Loss 3.32  | Δw: 57.362\n",
            "Iteration: 4565  | Loss 3.44  | Δw: 54.364\n",
            "Iteration: 4570  | Loss 3.48  | Δw: 57.072\n",
            "Iteration: 4575  | Loss 3.41  | Δw: 53.683\n",
            "Iteration: 4580  | Loss 3.24  | Δw: 52.587\n",
            "Iteration: 4585  | Loss 3.28  | Δw: 58.44\n",
            "Iteration: 4590  | Loss 3.44  | Δw: 55.531\n",
            "Iteration: 4595  | Loss 3.5  | Δw: 56.551\n",
            "Iteration: 4600  | Loss 3.26  | Δw: 53.329\n",
            "Iteration: 4605  | Loss 3.39  | Δw: 51.763\n",
            "Iteration: 4610  | Loss 3.2  | Δw: 51.311\n",
            "Iteration: 4615  | Loss 3.36  | Δw: 51.953\n",
            "Iteration: 4620  | Loss 3.57  | Δw: 53.082\n",
            "Iteration: 4625  | Loss 3.54  | Δw: 54.53\n",
            "Iteration: 4630  | Loss 3.33  | Δw: 58.365\n",
            "Iteration: 4635  | Loss 3.21  | Δw: 54.545\n",
            "Iteration: 4640  | Loss 3.09  | Δw: 55.196\n",
            "Iteration: 4645  | Loss 3.26  | Δw: 58.418\n",
            "Iteration: 4650  | Loss 3.21  | Δw: 51.869\n",
            "Iteration: 4655  | Loss 3.39  | Δw: 50.882\n",
            "Iteration: 4660  | Loss 3.2  | Δw: 55.103\n",
            "Iteration: 4665  | Loss 3.39  | Δw: 54.76\n",
            "Iteration: 4670  | Loss 3.21  | Δw: 49.256\n",
            "Iteration: 4675  | Loss 3.31  | Δw: 58.845\n",
            "Iteration: 4680  | Loss 3.49  | Δw: 50.039\n",
            "Iteration: 4685  | Loss 3.44  | Δw: 55.181\n",
            "Iteration: 4690  | Loss 3.63  | Δw: 54.174\n",
            "Iteration: 4695  | Loss 3.07  | Δw: 48.939\n",
            "Iteration: 4700  | Loss 3.21  | Δw: 53.244\n",
            "Iteration: 4705  | Loss 3.18  | Δw: 52.063\n",
            "Iteration: 4710  | Loss 3.29  | Δw: 56.339\n",
            "Iteration: 4715  | Loss 3.11  | Δw: 51.696\n",
            "Iteration: 4720  | Loss 3.61  | Δw: 53.25\n",
            "Iteration: 4725  | Loss 3.34  | Δw: 54.095\n",
            "Iteration: 4730  | Loss 3.07  | Δw: 53.658\n",
            "Iteration: 4735  | Loss 3.37  | Δw: 51.547\n",
            "Iteration: 4740  | Loss 3.47  | Δw: 55.985\n",
            "Iteration: 4745  | Loss 3.3  | Δw: 55.106\n",
            "Iteration: 4750  | Loss 3.52  | Δw: 55.802\n",
            "Iteration: 4755  | Loss 3.34  | Δw: 55.735\n",
            "Iteration: 4760  | Loss 3.44  | Δw: 56.264\n",
            "Iteration: 4765  | Loss 3.46  | Δw: 55.253\n",
            "Iteration: 4770  | Loss 3.44  | Δw: 53.697\n",
            "Iteration: 4775  | Loss 3.41  | Δw: 57.653\n",
            "Iteration: 4780  | Loss 3.15  | Δw: 52.591\n",
            "Iteration: 4785  | Loss 3.67  | Δw: 52.478\n",
            "Iteration: 4790  | Loss 3.42  | Δw: 55.777\n",
            "Iteration: 4795  | Loss 3.29  | Δw: 52.957\n",
            "Iteration: 4800  | Loss 3.56  | Δw: 54.267\n",
            "Iteration: 4805  | Loss 3.38  | Δw: 56.505\n",
            "Iteration: 4810  | Loss 2.94  | Δw: 50.55\n",
            "Iteration: 4815  | Loss 3.24  | Δw: 51.199\n",
            "Iteration: 4820  | Loss 3.64  | Δw: 59.407\n",
            "Iteration: 4825  | Loss 3.53  | Δw: 59.306\n",
            "Iteration: 4830  | Loss 3.52  | Δw: 53.494\n",
            "Iteration: 4835  | Loss 3.24  | Δw: 51.15\n",
            "Iteration: 4840  | Loss 3.45  | Δw: 52.248\n",
            "Iteration: 4845  | Loss 3.5  | Δw: 55.343\n",
            "Iteration: 4850  | Loss 3.34  | Δw: 53.965\n",
            "Iteration: 4855  | Loss 3.38  | Δw: 52.605\n",
            "Iteration: 4860  | Loss 3.19  | Δw: 55.655\n",
            "Iteration: 4865  | Loss 3.36  | Δw: 53.82\n",
            "Iteration: 4870  | Loss 3.36  | Δw: 55.31\n",
            "Iteration: 4875  | Loss 3.47  | Δw: 54.258\n",
            "Iteration: 4880  | Loss 3.23  | Δw: 53.686\n",
            "Iteration: 4885  | Loss 3.5  | Δw: 54.898\n",
            "Iteration: 4890  | Loss 3.17  | Δw: 55.368\n",
            "Iteration: 4895  | Loss 3.34  | Δw: 54.086\n",
            "Iteration: 4900  | Loss 3.29  | Δw: 53.407\n",
            "Iteration: 4905  | Loss 3.21  | Δw: 53.49\n",
            "Iteration: 4910  | Loss 3.32  | Δw: 55.056\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-72b3cdc0efc6>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmasked_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasked_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moutput_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-040481552107>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m                                        \u001b[0;31m# Add positional encoding to 3d matrix from line above, eg: [batch, word, embedding + pos_enc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoders\u001b[0m\u001b[0;34m:\u001b[0m                             \u001b[0;31m# Iterate through list of Multi-Attention Heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m                                        \u001b[0;31m# ToDo: Determine shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m                                            \u001b[0;31m# Applies normalization to the last dimension of the tensor; [batch, word, normalized vector representation]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m                                        \u001b[0;31m# Applies linear layer to each word embedding; [128, 20, 40000]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-4c70da35127b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                                            \u001b[0;31m# x is a 3d tensor of shape [batch, seq_len, embedding]; [128, 20, 128]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m                                                      \u001b[0;31m# Normalize the embedding vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                          \u001b[0;31m# Apply attention to the embedding vectors and then add to themselves with dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m                                                      \u001b[0;31m# Renormalize the embedding vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                                      \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-ceebdd10e799>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, mask)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m#break into n_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m        \u001b[0;31m# Reshape 3d to 4d tensor [batches, words, # of heads, # dim per head]; eg: [128, 20, 8, 16]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m           \u001b[0;31m# Transpose words and # of heads [ batches, t(# of heads), t(words), # dim per heads]; eg: [128, 8, 20, 16]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# Apply attention mechanism to q,k,v; [128, 8, 20, 16]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-ceebdd10e799>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m#break into n_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m        \u001b[0;31m# Reshape 3d to 4d tensor [batches, words, # of heads, # dim per head]; eg: [128, 20, 8, 16]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m           \u001b[0;31m# Transpose words and # of heads [ batches, t(# of heads), t(words), # dim per heads]; eg: [128, 8, 20, 16]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# Apply attention mechanism to q,k,v; [128, 8, 20, 16]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Evaluate Model"
      ],
      "metadata": {
        "id": "mAmt1E7Gdjtg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAmH7CF6NFBY"
      },
      "outputs": [],
      "source": [
        "print(\"Saving embeddings...\")\n",
        "N = 3000\n",
        "np.savetxt(\"values.tsv\", np.round(model.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter=\"\\t\", fmt=\"%1.2f\")\n",
        "s = [dataset.rvocab[i] for i in range(N)]\n",
        "open(\"names.tsv\", \"w+\").write(\"\\n\".join(s))\n",
        "\n",
        "print(\"End.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "https://hyugen-ai.medium.com/transformers-in-pytorch-from-scratch-for-nlp-beginners-ff3b3d922ef7  \n",
        "https://github.com/Whiax/BERT-Transformer-Pytorch  \n"
      ],
      "metadata": {
        "id": "pEZ6jU6rYxp4"
      }
    }
  ]
}